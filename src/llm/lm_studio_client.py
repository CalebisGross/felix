"""
LM Studio client integration for Felix Framework.

This module provides a client interface to LM Studio's OpenAI-compatible API,
enabling LLM-powered agents in the Felix multi-agent system.

LM Studio runs a local server (typically http://localhost:1234/v1) that 
provides OpenAI-compatible API endpoints for local language model inference.
"""

import asyncio
import time
import logging
from typing import Optional, Dict, Any, List, Union, Callable
from dataclasses import dataclass
from enum import Enum
from openai import OpenAI
import httpx
from collections import deque

logger = logging.getLogger(__name__)


class RequestPriority(Enum):
    """Priority levels for async requests."""
    LOW = 1
    NORMAL = 2
    HIGH = 3
    URGENT = 4


@dataclass
class AsyncRequest:
    """Async request with priority and metadata."""
    agent_id: str
    system_prompt: str
    user_prompt: str
    temperature: float
    max_tokens: Optional[int]
    model: str
    priority: RequestPriority
    future: asyncio.Future
    timestamp: float


@dataclass
class LLMResponse:
    """Response from LLM completion."""
    content: str
    tokens_used: int
    response_time: float
    model: str
    temperature: float
    agent_id: str
    timestamp: float


@dataclass
class StreamingChunk:
    """Incremental streaming chunk from LLM with time-batching."""
    content: str           # Partial text since last batch
    accumulated: str       # Full content accumulated so far
    tokens_so_far: int     # Running token count (approximate)
    agent_id: str
    timestamp: float


class TokenAwareStreamController:
    """
    Controls streaming LLM generation with real-time token budget awareness.

    Monitors token usage during streaming and provides graceful conclusion
    signals when approaching budget limits, preventing abrupt truncation
    while maintaining output quality.

    Features:
    - Real-time token counting during generation
    - Soft limit (85%) triggers conclusion signal
    - Hard limit (100%) stops stream
    - Token efficiency metrics for agent learning
    """

    def __init__(self, token_budget: int, soft_limit_ratio: float = 0.85,
                 conclusion_signal: Optional[str] = None):
        """
        Initialize token-aware stream controller.

        Args:
            token_budget: Maximum tokens allowed for this generation
            soft_limit_ratio: Ratio at which to inject conclusion signal (default 0.85)
            conclusion_signal: Optional signal to inject at soft limit
        """
        self.token_budget = token_budget
        self.soft_limit = int(token_budget * soft_limit_ratio)
        self.hard_limit = token_budget
        self.tokens_generated = 0
        self.should_stop = False
        self.conclusion_injected = False

        # Default conclusion signal (subtle)
        self.conclusion_signal = conclusion_signal or ""

        logger.debug(f"TokenAwareStreamController initialized: "
                    f"budget={token_budget}, soft={self.soft_limit}, hard={self.hard_limit}")

    def process_chunk(self, chunk: str) -> tuple[str, bool]:
        """
        Process streaming chunk and determine if generation should continue.

        Uses rough token estimation: ~0.75 words per token (1.33 tokens per word).
        More accurate than character count, accounts for multi-byte tokens.

        Args:
            chunk: Text chunk from LLM stream

        Returns:
            Tuple of (processed_chunk, should_continue)
            - processed_chunk: Chunk potentially modified with signals
            - should_continue: False to stop streaming, True to continue
        """
        # Estimate tokens in this chunk (rough but effective)
        # Average: 1 token â‰ˆ 0.75 words, so 1 word â‰ˆ 1.33 tokens
        word_count = len(chunk.split())
        estimated_tokens = int(word_count * 1.33)
        self.tokens_generated += estimated_tokens

        # HARD LIMIT - stop immediately, don't emit this chunk
        if self.tokens_generated >= self.hard_limit:
            logger.debug(f"TokenAwareStreamController: HARD LIMIT reached "
                        f"({self.tokens_generated}/{self.hard_limit} tokens) - stopping stream")
            return "", False

        # SOFT LIMIT - inject conclusion signal once, continue briefly
        elif self.tokens_generated >= self.soft_limit and not self.conclusion_injected:
            logger.debug(f"TokenAwareStreamController: SOFT LIMIT reached "
                        f"({self.tokens_generated}/{self.soft_limit} tokens) - injecting conclusion signal")
            self.conclusion_injected = True
            self.should_stop = True

            # Inject conclusion signal if provided
            if self.conclusion_signal:
                return chunk + self.conclusion_signal, True

            return chunk, True

        # Normal operation - continue streaming
        return chunk, True

    def get_efficiency_ratio(self) -> float:
        """
        Calculate token efficiency ratio (used / allocated).

        Returns:
            Efficiency ratio (e.g., 0.85 means used 85% of budget)
        """
        if self.token_budget == 0:
            return 1.0
        return self.tokens_generated / self.token_budget

    def get_metrics(self) -> Dict[str, Any]:
        """
        Get token usage metrics for agent learning.

        Returns:
            Dictionary with token usage statistics
        """
        return {
            "tokens_allocated": self.token_budget,
            "tokens_used": self.tokens_generated,
            "efficiency_ratio": self.get_efficiency_ratio(),
            "soft_limit_reached": self.conclusion_injected,
            "hard_limit_reached": self.tokens_generated >= self.hard_limit,
            "tokens_remaining": max(0, self.token_budget - self.tokens_generated)
        }


class LMStudioConnectionError(Exception):
    """Raised when cannot connect to LM Studio."""
    pass


class LMStudioClient:
    """
    Client for communicating with LM Studio's local API server.
    
    Provides both synchronous and asynchronous methods for LLM completion,
    with built-in error handling, connection testing, and usage tracking.
    """
    
    def __init__(self, base_url: str = "http://localhost:1234/v1",
                 timeout: float = 120.0, max_concurrent_requests: int = 4,
                 debug_mode: bool = False, verbose_logging: bool = True):
        """
        Initialize LM Studio client.

        Args:
            base_url: LM Studio API endpoint
            timeout: Request timeout in seconds
            max_concurrent_requests: Maximum concurrent async requests
            debug_mode: Enable verbose debug output to console
            verbose_logging: Enable detailed logging to Felix logging system (for GUI)
        """
        self.base_url = base_url
        self.timeout = timeout
        self.max_concurrent_requests = max_concurrent_requests
        self.debug_mode = debug_mode
        self.verbose_logging = verbose_logging

        # DEBUG: Confirm initialization
        print(f"DEBUG: LMStudioClient created, verbose_logging={verbose_logging}")

        # Ensure logger is configured
        logger.setLevel(logging.INFO)
        logger.propagate = True

        # Log initialization with verbose setting
        logger.info(f"LMStudioClient initialized: base_url={base_url}, verbose_logging={verbose_logging}")

        # Sync client
        self.client = OpenAI(
            base_url=base_url,
            api_key="not-needed",  # LM Studio doesn't require API keys
            timeout=timeout
        )
        
        # Async client and connection pool
        self._async_client: Optional[httpx.AsyncClient] = None
        self._connection_semaphore = asyncio.Semaphore(max_concurrent_requests)
        
        # Request queue for async processing
        self._request_queue: deque[AsyncRequest] = deque()
        self._queue_processor_task: Optional[asyncio.Task] = None
        self._is_processing_queue = False
        
        # Usage tracking
        self.total_tokens = 0
        self.total_requests = 0
        self.total_response_time = 0.0
        self.concurrent_requests = 0
        
        # Connection state
        self._connection_verified = False
    
    def test_connection(self) -> bool:
        """
        Test connection to LM Studio server.
        
        Returns:
            True if connection successful, False otherwise
        """
        try:
            response = httpx.get(f"{self.base_url}/models", timeout=5.0)
            self._connection_verified = response.status_code == 200
            return self._connection_verified
        except Exception as e:
            logger.warning(f"LM Studio connection test failed: {e}")
            return False
    
    def ensure_connection(self) -> None:
        """Ensure connection to LM Studio or raise exception."""
        if not self._connection_verified and not self.test_connection():
            raise LMStudioConnectionError(
                f"Cannot connect to LM Studio at {self.base_url}. "
                "Ensure LM Studio is running with a model loaded."
            )
    
    def complete(self, agent_id: str, system_prompt: str, user_prompt: str,
                 temperature: float = 0.7, max_tokens: Optional[int] = 500,
                 model: str = "local-model") -> LLMResponse:
        """
        Synchronous completion request to LM Studio.
        
        Args:
            agent_id: Identifier for the requesting agent
            system_prompt: System/context prompt
            user_prompt: User query/task
            temperature: Sampling temperature (0.0-1.0)
            max_tokens: Maximum tokens in response
            model: Model identifier (LM Studio will use loaded model)
            
        Returns:
            LLMResponse with content and metadata
            
        Raises:
            LMStudioConnectionError: If cannot connect to LM Studio
        """
        self.ensure_connection()
        
        start_time = time.perf_counter()

        try:
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]

            # Log request details at INFO level for GUI visibility (if enabled)
            if self.verbose_logging:
                system_preview = system_prompt[:100] + "..." if len(system_prompt) > 100 else system_prompt
                user_preview = user_prompt[:100] + "..." if len(user_prompt) > 100 else user_prompt

                logger.info("=" * 60)
                logger.info(f"LLM REQUEST to LM Studio")
                logger.info(f"  Agent: {agent_id}")
                logger.info(f"  Model: {model}")
                logger.info(f"  Temperature: {temperature}, Max Tokens: {max_tokens}")
                logger.info(f"  System Prompt ({len(system_prompt)} chars): {system_preview}")
                logger.info(f"  User Prompt ({len(user_prompt)} chars): {user_preview}")
                logger.info("=" * 60)

            if self.debug_mode:
                print(f"\nğŸ” DEBUG LLM CALL for {agent_id}")
                print(f"ğŸ“ System Prompt:\n{system_prompt}")
                print(f"ğŸ¯ User Prompt:\n{user_prompt}")
                print(f"ğŸŒ¡ï¸ Temperature: {temperature}, Max Tokens: {max_tokens}")
                print("â”" * 60)

            completion_args = {
                "model": model,
                "messages": messages,
                "temperature": temperature
            }

            if max_tokens:
                completion_args["max_tokens"] = max_tokens

            response = self.client.chat.completions.create(**completion_args)
            
            end_time = time.perf_counter()
            response_time = end_time - start_time

            # Extract response data
            content = response.choices[0].message.content
            tokens_used = response.usage.total_tokens if response.usage else 0
            prompt_tokens = response.usage.prompt_tokens if response.usage else 0
            completion_tokens = response.usage.completion_tokens if response.usage else 0

            # Update usage tracking
            self.total_tokens += tokens_used
            self.total_requests += 1
            self.total_response_time += response_time

            # Log response details at INFO level for GUI visibility (if enabled)
            if self.verbose_logging:
                content_preview = content[:200] + "..." if len(content) > 200 else content

                logger.info("=" * 60)
                logger.info(f"LLM RESPONSE from LM Studio")
                logger.info(f"  Agent: {agent_id}")
                logger.info(f"  Model: {model}")
                logger.info(f"  Response Time: {response_time:.2f}s")
                logger.info(f"  Tokens: {tokens_used} total (prompt: {prompt_tokens}, completion: {completion_tokens})")
                logger.info(f"  Content ({len(content)} chars): {content_preview}")
                logger.info("=" * 60)

            if self.debug_mode:
                print(f"âœ… LLM RESPONSE for {agent_id}")
                print(f"ğŸ“„ Content ({len(content)} chars):\n{content}")
                print(f"ğŸ“Š Tokens Used: {tokens_used}, Time: {response_time:.2f}s")
                print("â”" * 60)
            
            return LLMResponse(
                content=content,
                tokens_used=tokens_used,
                response_time=response_time,
                model=model,
                temperature=temperature,
                agent_id=agent_id,
                timestamp=time.time()
            )
            
        except Exception as e:
            logger.error(f"LLM completion failed for {agent_id}: {e}")
            raise

    def generate_embedding(self, text: str, model: str = "local-model") -> Optional[List[float]]:
        """
        Generate embedding vector for text using LM Studio's embedding endpoint.

        LM Studio supports OpenAI-compatible embeddings API when an embedding-capable
        model is loaded (e.g., text-embedding-ada-002, nomic-embed-text).

        Args:
            text: Input text to embed
            model: Model identifier (LM Studio will use loaded embedding model)

        Returns:
            List of floats (embedding vector) or None if failed

        Raises:
            LMStudioConnectionError: If cannot connect to LM Studio
        """
        self.ensure_connection()

        try:
            # Use OpenAI-compatible embeddings API
            response = self.client.embeddings.create(
                model=model,
                input=text
            )

            # Extract embedding from response
            if response.data and len(response.data) > 0:
                embedding = response.data[0].embedding

                if self.verbose_logging:
                    logger.info(f"Generated embedding for text ({len(text)} chars), "
                               f"dimension: {len(embedding)}")

                return embedding

            logger.warning("LM Studio embedding response contained no data")
            return None

        except Exception as e:
            # This is expected if LM Studio doesn't have an embedding model loaded
            logger.debug(f"Embedding generation failed: {e}")
            return None

    def complete_streaming(
        self,
        agent_id: str,
        system_prompt: str,
        user_prompt: str,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        model: str = "local-model",
        batch_interval: float = 0.1,
        callback: Optional[Callable[[StreamingChunk], None]] = None,
        token_controller: Optional[TokenAwareStreamController] = None
    ) -> LLMResponse:
        """
        Stream LLM completion with time-batched token delivery and optional budget control.

        Tokens are accumulated and sent to callback every `batch_interval` seconds
        to create real-time feel without excessive overhead.

        Args:
            agent_id: Identifier for requesting agent
            system_prompt: System/context prompt
            user_prompt: User query/task
            temperature: Sampling temperature (0.0-1.0)
            max_tokens: Maximum tokens in response
            model: Model identifier
            batch_interval: Time between partial updates (default 0.1s = 100ms)
            callback: Called with batched chunks for real-time updates
            token_controller: Optional TokenAwareStreamController for budget enforcement

        Returns:
            Complete LLMResponse after stream finishes

        Raises:
            LMStudioConnectionError: If cannot connect to LM Studio
        """
        self.ensure_connection()

        start_time = time.perf_counter()
        accumulated_content = ""
        batch_buffer = ""  # Buffer for time-based batching
        last_batch_time = time.time()
        tokens_so_far = 0

        try:
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]

            if self.verbose_logging:
                system_preview = system_prompt[:100] + "..." if len(system_prompt) > 100 else system_prompt
                user_preview = user_prompt[:100] + "..." if len(user_prompt) > 100 else user_prompt

                logger.info("=" * 60)
                logger.info(f"STREAMING LLM REQUEST (time-batched, interval={batch_interval}s)")
                logger.info(f"  Agent: {agent_id}")
                logger.info(f"  Model: {model}")
                logger.info(f"  Temperature: {temperature}, Max Tokens: {max_tokens}")
                logger.info(f"  System Prompt ({len(system_prompt)} chars): {system_preview}")
                logger.info(f"  User Prompt ({len(user_prompt)} chars): {user_preview}")
                logger.info("=" * 60)

            if self.debug_mode:
                print(f"\nğŸ” DEBUG STREAMING LLM CALL for {agent_id}")
                print(f"ğŸ“ System Prompt:\n{system_prompt}")
                print(f"ğŸ¯ User Prompt:\n{user_prompt}")
                print(f"ğŸŒ¡ï¸ Temperature: {temperature}, Max Tokens: {max_tokens}")
                print(f"â±ï¸ Batch Interval: {batch_interval}s")
                print("â”" * 60)

            # Create streaming completion using OpenAI client
            # LM Studio supports this on /v1/chat/completions endpoint
            completion_args = {
                "model": model,
                "messages": messages,
                "temperature": temperature,
                "stream": True  # Enable streaming!
            }

            if max_tokens:
                completion_args["max_tokens"] = max_tokens

            stream = self.client.chat.completions.create(**completion_args)

            # Process stream with time-based batching and optional token control
            for chunk in stream:
                if chunk.choices and len(chunk.choices) > 0 and chunk.choices[0].delta.content:
                    delta_content = chunk.choices[0].delta.content

                    # Token-aware processing (if controller provided)
                    if token_controller:
                        processed_chunk, should_continue = token_controller.process_chunk(delta_content)
                        if not should_continue:
                            logger.info(f"Token controller stopped stream for {agent_id} "
                                       f"at {token_controller.tokens_generated} tokens")
                            # Don't add the rejected chunk, break immediately
                            break
                        # Use processed chunk (may have conclusion signal injected)
                        delta_content = processed_chunk

                    accumulated_content += delta_content
                    batch_buffer += delta_content
                    tokens_so_far += 1  # Approximate (1 token â‰ˆ 1 chunk)

                    # Check if batch interval elapsed
                    current_time = time.time()
                    if current_time - last_batch_time >= batch_interval:
                        # Send batched chunk via callback
                        if callback and batch_buffer:
                            streaming_chunk = StreamingChunk(
                                content=batch_buffer,
                                accumulated=accumulated_content,
                                tokens_so_far=tokens_so_far,
                                agent_id=agent_id,
                                timestamp=current_time
                            )
                            callback(streaming_chunk)

                        # Reset batch
                        batch_buffer = ""
                        last_batch_time = current_time

            # Send final batch if remaining
            if batch_buffer and callback:
                streaming_chunk = StreamingChunk(
                    content=batch_buffer,
                    accumulated=accumulated_content,
                    tokens_so_far=tokens_so_far,
                    agent_id=agent_id,
                    timestamp=time.time()
                )
                callback(streaming_chunk)

            end_time = time.perf_counter()
            response_time = end_time - start_time

            # Update tracking
            self.total_tokens += tokens_so_far
            self.total_requests += 1
            self.total_response_time += response_time

            if self.verbose_logging:
                content_preview = accumulated_content[:200] + "..." if len(accumulated_content) > 200 else accumulated_content

                logger.info("=" * 60)
                logger.info(f"STREAMING LLM COMPLETE")
                logger.info(f"  Agent: {agent_id}")
                logger.info(f"  Duration: {response_time:.2f}s")
                logger.info(f"  Tokens: ~{tokens_so_far}")
                logger.info(f"  Content ({len(accumulated_content)} chars): {content_preview}")
                logger.info("=" * 60)

            if self.debug_mode:
                print(f"âœ… STREAMING LLM COMPLETE for {agent_id}")
                print(f"ğŸ“„ Content ({len(accumulated_content)} chars):\n{accumulated_content}")
                print(f"ğŸ“Š Tokens Used: ~{tokens_so_far}, Time: {response_time:.2f}s")
                print("â”" * 60)

            return LLMResponse(
                content=accumulated_content,
                tokens_used=tokens_so_far,
                response_time=response_time,
                model=model,
                temperature=temperature,
                agent_id=agent_id,
                timestamp=time.time()
            )

        except Exception as e:
            logger.error(f"Streaming LLM completion failed for {agent_id}: {e}")
            raise

    async def _ensure_async_client(self) -> httpx.AsyncClient:
        """Ensure async client is initialized."""
        if self._async_client is None:
            limits = httpx.Limits(
                max_connections=self.max_concurrent_requests,
                max_keepalive_connections=self.max_concurrent_requests
            )
            timeout = httpx.Timeout(self.timeout)
            
            self._async_client = httpx.AsyncClient(
                base_url=self.base_url,
                timeout=timeout,
                limits=limits,
                headers={"Content-Type": "application/json"}
            )
        return self._async_client
    
    async def _make_async_request(self, agent_id: str, system_prompt: str, 
                                user_prompt: str, temperature: float = 0.7,
                                max_tokens: Optional[int] = None,
                                model: str = "local-model") -> LLMResponse:
        """Make actual async HTTP request to LM Studio."""
        async with self._connection_semaphore:
            start_time = time.perf_counter()
            self.concurrent_requests += 1
            
            try:
                client = await self._ensure_async_client()
                
                messages = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ]

                # Log request details at INFO level for GUI visibility (if enabled)
                if self.verbose_logging:
                    system_preview = system_prompt[:100] + "..." if len(system_prompt) > 100 else system_prompt
                    user_preview = user_prompt[:100] + "..." if len(user_prompt) > 100 else user_prompt

                    logger.info("=" * 60)
                    logger.info(f"ASYNC LLM REQUEST to LM Studio")
                    logger.info(f"  Agent: {agent_id}")
                    logger.info(f"  Model: {model}")
                    logger.info(f"  Temperature: {temperature}, Max Tokens: {max_tokens}")
                    logger.info(f"  System Prompt ({len(system_prompt)} chars): {system_preview}")
                    logger.info(f"  User Prompt ({len(user_prompt)} chars): {user_preview}")
                    logger.info("=" * 60)

                if self.debug_mode:
                    print(f"\nğŸ” DEBUG ASYNC LLM CALL for {agent_id}")
                    print(f"ğŸ“ System Prompt:\n{system_prompt}")
                    print(f"ğŸ¯ User Prompt:\n{user_prompt}")
                    print(f"ğŸŒ¡ï¸ Temperature: {temperature}, Max Tokens: {max_tokens}")
                    print("â”" * 60)

                payload = {
                    "model": model,
                    "messages": messages,
                    "temperature": temperature,
                    "stream": False
                }

                if max_tokens:
                    payload["max_tokens"] = max_tokens

                response = await client.post("/chat/completions", json=payload)
                response.raise_for_status()

                data = response.json()

                end_time = time.perf_counter()
                response_time = end_time - start_time

                # Extract response data
                content = data["choices"][0]["message"]["content"]
                tokens_used = data.get("usage", {}).get("total_tokens", 0)
                prompt_tokens = data.get("usage", {}).get("prompt_tokens", 0)
                completion_tokens = data.get("usage", {}).get("completion_tokens", 0)

                # Update usage tracking
                self.total_tokens += tokens_used
                self.total_requests += 1
                self.total_response_time += response_time

                # Log response details at INFO level for GUI visibility (if enabled)
                if self.verbose_logging:
                    content_preview = content[:200] + "..." if len(content) > 200 else content

                    logger.info("=" * 60)
                    logger.info(f"ASYNC LLM RESPONSE from LM Studio")
                    logger.info(f"  Agent: {agent_id}")
                    logger.info(f"  Model: {model}")
                    logger.info(f"  Response Time: {response_time:.2f}s")
                    logger.info(f"  Tokens: {tokens_used} total (prompt: {prompt_tokens}, completion: {completion_tokens})")
                    logger.info(f"  Content ({len(content)} chars): {content_preview}")
                    logger.info("=" * 60)

                if self.debug_mode:
                    print(f"âœ… ASYNC LLM RESPONSE for {agent_id}")
                    print(f"ğŸ“„ Content ({len(content)} chars):\n{content}")
                    print(f"ğŸ“Š Tokens Used: {tokens_used}, Time: {response_time:.2f}s")
                    print("â”" * 60)
                
                return LLMResponse(
                    content=content,
                    tokens_used=tokens_used,
                    response_time=response_time,
                    model=model,
                    temperature=temperature,
                    agent_id=agent_id,
                    timestamp=time.time()
                )
                
            except Exception as e:
                logger.error(f"Async LLM completion failed for {agent_id}: {e}")
                raise
            finally:
                self.concurrent_requests -= 1
    
    async def complete_async(self, agent_id: str, system_prompt: str, 
                           user_prompt: str, temperature: float = 0.7,
                           max_tokens: Optional[int] = None,
                           model: str = "local-model",
                           priority: RequestPriority = RequestPriority.NORMAL) -> LLMResponse:
        """
        Asynchronous completion request to LM Studio with priority support.
        
        Args:
            agent_id: Identifier for the requesting agent
            system_prompt: System/context prompt
            user_prompt: User query/task
            temperature: Sampling temperature (0.0-1.0)
            max_tokens: Maximum tokens in response
            model: Model identifier
            priority: Request priority level
            
        Returns:
            LLMResponse with content and metadata
        """
        # For high priority requests, execute immediately
        if priority == RequestPriority.URGENT:
            return await self._make_async_request(
                agent_id, system_prompt, user_prompt, temperature, max_tokens, model
            )
        
        # For normal/low priority, use queue system
        future = asyncio.Future()
        request = AsyncRequest(
            agent_id=agent_id,
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            model=model,
            priority=priority,
            future=future,
            timestamp=time.time()
        )
        
        self._request_queue.append(request)
        await self._ensure_queue_processor()
        
        return await future
    
    def get_usage_stats(self) -> Dict[str, Any]:
        """
        Get client usage statistics.
        
        Returns:
            Dictionary with usage metrics
        """
        avg_response_time = (self.total_response_time / self.total_requests 
                           if self.total_requests > 0 else 0.0)
        
        return {
            "total_requests": self.total_requests,
            "total_tokens": self.total_tokens,
            "total_response_time": self.total_response_time,
            "average_response_time": avg_response_time,
            "average_tokens_per_request": (self.total_tokens / self.total_requests
                                         if self.total_requests > 0 else 0.0),
            "connection_verified": self._connection_verified,
            "max_concurrent_requests": self.max_concurrent_requests,
            "current_concurrent_requests": self.concurrent_requests,
            "queue_size": len(self._request_queue)
        }
    
    async def _ensure_queue_processor(self) -> None:
        """Ensure queue processor task is running."""
        if self._queue_processor_task is None or self._queue_processor_task.done():
            self._queue_processor_task = asyncio.create_task(self._process_request_queue())
    
    async def _process_request_queue(self) -> None:
        """Process queued async requests with priority ordering."""
        if self._is_processing_queue:
            return
        
        self._is_processing_queue = True
        
        try:
            while self._request_queue:
                # Sort queue by priority (higher priority first)
                sorted_requests = sorted(self._request_queue, key=lambda r: r.priority.value, reverse=True)
                
                # Process up to max_concurrent_requests at once
                batch_size = min(len(sorted_requests), self.max_concurrent_requests)
                batch = [sorted_requests[i] for i in range(batch_size)]
                
                # Remove processed requests from queue
                for req in batch:
                    self._request_queue.remove(req)
                
                # Process batch concurrently
                tasks = []
                for req in batch:
                    task = asyncio.create_task(self._execute_queued_request(req))
                    tasks.append(task)
                
                if tasks:
                    await asyncio.gather(*tasks, return_exceptions=True)
                
                # Small delay to prevent busy waiting
                if not self._request_queue:
                    break
                await asyncio.sleep(0.01)
                    
        finally:
            self._is_processing_queue = False
    
    async def _execute_queued_request(self, request: AsyncRequest) -> None:
        """Execute a single queued request."""
        try:
            result = await self._make_async_request(
                request.agent_id,
                request.system_prompt, 
                request.user_prompt,
                request.temperature,
                request.max_tokens,
                request.model
            )
            request.future.set_result(result)
        except Exception as e:
            request.future.set_exception(e)
    
    async def close_async(self) -> None:
        """Close async client and cleanup resources."""
        if self._async_client:
            await self._async_client.aclose()
            self._async_client = None
        
        if self._queue_processor_task and not self._queue_processor_task.done():
            self._queue_processor_task.cancel()
            try:
                await self._queue_processor_task
            except asyncio.CancelledError:
                pass
    
    def reset_stats(self) -> None:
        """Reset usage statistics."""
        self.total_tokens = 0
        self.total_requests = 0
        self.total_response_time = 0.0
    
    def create_agent_system_prompt(self, agent_type: str, position_info: Dict[str, float],
                                 task_context: str = "") -> str:
        """
        Create system prompt for Felix agent based on position and type.
        
        Args:
            agent_type: Type of agent (research, analysis, synthesis, critic)
            position_info: Agent's position on helix (x, y, z, radius, depth_ratio)
            task_context: Additional context about the current task
            
        Returns:
            Formatted system prompt
        """
        depth_ratio = position_info.get("depth_ratio", 0.0)
        radius = position_info.get("radius", 0.0)
        
        base_prompt = f"""âš ï¸âš ï¸âš ï¸ CRITICAL TOOLS AVAILABLE âš ï¸âš ï¸âš ï¸

ğŸ” WEB SEARCH - USE THIS FOR CURRENT INFORMATION:
If you need current/real-time data (dates, times, recent events, latest stats), write EXACTLY:
WEB_SEARCH_NEEDED: [your query]

EXAMPLES - COPY THIS FORMAT:
âœ“ Time query: "WEB_SEARCH_NEEDED: current date and time"
âœ“ Recent event: "WEB_SEARCH_NEEDED: 2024 election results"
âœ“ Latest stat: "WEB_SEARCH_NEEDED: current inflation rate"

ğŸš¨ DO NOT say "I cannot access" - REQUEST A SEARCH FIRST!

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ–¥ï¸ SYSTEM COMMANDS - USE THIS FOR SYSTEM OPERATIONS:
If you need to check system state, run commands, or interact with the terminal, write EXACTLY:
SYSTEM_ACTION_NEEDED: [command]

EXAMPLES - COPY THIS FORMAT:
âœ“ Check time/date: "SYSTEM_ACTION_NEEDED: date"
âœ“ Check directory: "SYSTEM_ACTION_NEEDED: pwd"
âœ“ List files: "SYSTEM_ACTION_NEEDED: ls -la"
âœ“ Check Python packages: "SYSTEM_ACTION_NEEDED: pip list"
âœ“ Activate venv: "SYSTEM_ACTION_NEEDED: source .venv/bin/activate"

SAFETY LEVELS:
- SAFE (execute immediately): ls, pwd, date, pip list, which, etc.
- REVIEW (need approval): pip install, git push, mkdir, etc.
- BLOCKED (never execute): rm -rf, dangerous operations

ğŸš¨ DO NOT say "I cannot access the terminal" - REQUEST A COMMAND FIRST!

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

You are a {agent_type} agent in Felix multi-agent system.

Position:
- Depth: {depth_ratio:.2f} (0.0=top, 1.0=bottom)
- Radius: {radius:.2f}
- Stage: {"Early/Broad" if depth_ratio < 0.3 else "Middle/Focused" if depth_ratio < 0.7 else "Final/Precise"}

âš ï¸ OUTPUT LIMIT: Response will be CUT OFF if too long. BE CONCISE.

Your Role:
"""
        
        if agent_type == "research":
            if depth_ratio < 0.3:
                base_prompt += "- MAXIMUM 5 bullet points with key facts ONLY\n"
                base_prompt += "- NO explanations, NO introductions, NO conclusions\n"
                base_prompt += "- Raw findings only - be direct\n"
            else:
                base_prompt += "- MAXIMUM 3 specific facts with numbers/dates/quotes\n"
                base_prompt += "- NO background context or elaboration\n"
                base_prompt += "- Prepare key points for analysis (concise)\n"
        
        elif agent_type == "analysis":
            base_prompt += "- MAXIMUM 2 numbered insights/patterns ONLY\n"
            base_prompt += "- NO background explanation or context\n"
            base_prompt += "- Direct analytical findings only\n"
            
        elif agent_type == "synthesis":
            base_prompt += "- FINAL output ONLY - NO process description\n"
            base_prompt += "- MAXIMUM 3 short paragraphs\n"
            base_prompt += "- Direct, actionable content without fluff\n"
            
        elif agent_type == "critic":
            base_prompt += "- MAXIMUM 3 specific issues/fixes ONLY\n"
            base_prompt += "- NO praise, NO general comments\n"
            base_prompt += "- Direct problems and solutions only\n"
        
        if task_context:
            base_prompt += f"\nTask Context: {task_context}\n"
        
        base_prompt += "\nğŸš¨ FINAL REMINDER: Your response must be EXTREMELY CONCISE. Your output will be CUT OFF if it exceeds your token limit. "
        base_prompt += "Early positions focus on breadth, later positions focus on depth and precision. BE BRIEF!"
        
        return base_prompt


def create_default_client(max_concurrent_requests: int = 4) -> LMStudioClient:
    """Create LM Studio client with default settings."""
    return LMStudioClient(max_concurrent_requests=max_concurrent_requests)